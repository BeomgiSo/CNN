{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.9 ('tensor2.0')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n tensor2.0 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TF: \", tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "import wandb\n",
    "print(\"W&B: \", wandb.__version__)\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import medmnist\n",
    "print(\"medMNIST: \", medmnist.__version__)\n",
    "from medmnist import INFO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgiso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    data_flag = 'bloodmnist',\n",
    "    image_width = 32,\n",
    "    image_height = 32,\n",
    "    batch_size = 128,\n",
    "    model_name = 'vgg16',\n",
    "    pretrain_weights = 'imagenet',\n",
    "    epochs = 100,\n",
    "    init_learning_rate = 0.001,\n",
    "    lr_decay_rate = 0.1,\n",
    "    optimizer = 'adam',\n",
    "    loss_fn = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['acc'],\n",
    "    earlystopping_patience = 5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_class': 'BloodMNIST',\n",
       " 'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.',\n",
       " 'url': 'https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1',\n",
       " 'MD5': '7053d0359d879ad8a5505303e11de1dc',\n",
       " 'task': 'multi-class',\n",
       " 'label': {'0': 'basophil',\n",
       "  '1': 'eosinophil',\n",
       "  '2': 'erythroblast',\n",
       "  '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)',\n",
       "  '4': 'lymphocyte',\n",
       "  '5': 'monocyte',\n",
       "  '6': 'neutrophil',\n",
       "  '7': 'platelet'},\n",
       " 'n_channels': 3,\n",
       " 'n_samples': {'train': 11959, 'val': 1712, 'test': 3421},\n",
       " 'license': 'CC BY 4.0'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = INFO[configs['data_flag']]\n",
    "configs['class_names'] = info['label']\n",
    "configs['image_channels'] = info['n_channels']\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"\n",
    "    Utility function to download the dataset and return train/valid/test images/labels.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata\n",
    "    \"\"\"\n",
    "    data_path = tf.keras.utils.get_file(origin=data_info['url'], md5_hash=data_info['MD5'])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get images\n",
    "        train_images = data['train_images']\n",
    "        valid_images = data['val_images']\n",
    "        test_images = data['test_images']\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data['train_labels'].flatten()\n",
    "        valid_labels = data['val_labels'].flatten()\n",
    "        test_labels = data['test_labels'].flatten()\n",
    "\n",
    "    return train_images, train_labels, valid_images, valid_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, valid_images, valid_labels, test_images, test_labels = download_and_prepare_dataset(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images : 1000 to be logged\n"
     ]
    }
   ],
   "source": [
    "# For demonstration purposes\n",
    "log_full = False #@param {type:\"boolean\"}\n",
    "\n",
    "if log_full:\n",
    "    log_train_samples = len(train_images)\n",
    "else:\n",
    "    log_train_samples = 1000 \n",
    "    \n",
    "\n",
    "print(f'Number of train images : {log_train_samples} to be logged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ramg/Desktop/project3_CNN/CNN/wandb/run-20220614_205510-3f9w3585</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3f9w3585\" target=\"_blank\">lucky-bush-8</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d885d453464a7b873ca1c89c669fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lucky-bush-8</strong>: <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3f9w3585\" target=\"_blank\">https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3f9w3585</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220614_205510-3f9w3585/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 791 ms, sys: 335 ms, total: 1.13 s\n",
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a new W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_data')\n",
    "\n",
    "# Intialize a W&B Artifacts\n",
    "ds = wandb.Artifact(\"medmnist_bloodmnist_dataset\", \"dataset\")\n",
    "\n",
    "# Initialize an empty table\n",
    "train_table = wandb.Table(columns=[], data=[])\n",
    "# Add training data\n",
    "train_table.add_column('image', train_images[:log_train_samples])\n",
    "# Add training label_id\n",
    "train_table.add_column('label_id', train_labels[:log_train_samples])\n",
    "# Add training class names\n",
    "train_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_names\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "\n",
    "# Add the table to the Artifact\n",
    "ds['train_data'] = train_table\n",
    "\n",
    "# Let's do the same for the validation data\n",
    "valid_table = wandb.Table(columns=[], data=[])\n",
    "valid_table.add_column('image', valid_images)\n",
    "valid_table.add_column('label_id', valid_labels)\n",
    "valid_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_name\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "ds['valid_data'] = valid_table\n",
    "\n",
    "# Save the dataset as an Artifact\n",
    "ds.save()\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@tf.function\n",
    "def preprocess(image: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Preprocess the image tensors and parse the labels\n",
    "    \"\"\"\n",
    "    # Preprocess images\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(images: np.ndarray,\n",
    "                       labels: np.ndarray,\n",
    "                       loader_type: str='train',\n",
    "                       batch_size: int=128):\n",
    "    \"\"\"\n",
    "    Utility function to prepare dataloader.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    if loader_type=='train':\n",
    "        dataset = dataset.shuffle(1024)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 20:55:20.233261: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-14 20:55:20.233399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "trainloader = prepare_dataloader(train_images, train_labels, 'train', configs.get('batch_size', 64))\n",
    "validloader = prepare_dataloader(valid_images, valid_labels, 'valid', configs.get('batch_size', 64))\n",
    "testloader = prepare_dataloader(test_images, test_labels, 'test', configs.get('batch_size', 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation = models.Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomFlip()],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def augment_5_times(img):\n",
    "    augmented_imgs = []\n",
    "    for _ in range(5):\n",
    "        aug_img = tf.squeeze(img_augmentation(img), axis=0)\n",
    "        # Notice the use of wrapping the images with wandb.Image\n",
    "        wandb_image = wandb.Image(aug_img.numpy())\n",
    "        augmented_imgs.append(wandb_image)\n",
    "\n",
    "    return augmented_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ramg/Desktop/project3_CNN/CNN/wandb/run-20220614_205520-3lbuzrai</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3lbuzrai\" target=\"_blank\">radiant-music-9</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotFoundError",
     "evalue": "Exception encountered when calling layer \"random_rotation\" (type RandomRotation).\n\nNo registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}\n\t.  Registered:  device='CPU'\n [Op:RngReadAndSkip]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 28, 28, 3), dtype=int64)\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/44/6_rc724s369980pjcph8pltr0000gp/T/ipykernel_13282/3458799867.py\u001b[0m in \u001b[0;36maugment_5_times\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0maugmented_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0maug_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Notice the use of wrapping the images with wandb.Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mwandb_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensor-mac/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensor-mac/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Exception encountered when calling layer \"random_rotation\" (type RandomRotation).\n\nNo registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}\n\t.  Registered:  device='CPU'\n [Op:RngReadAndSkip]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1, 28, 28, 3), dtype=int64)\n  • training=True"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "viz_augment_samples = 100\n",
    "\n",
    "# Initialize a W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_augmentation')\n",
    "\n",
    "# Use the already logged dataset\n",
    "train_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "\n",
    "# Get the train_table to access the data\n",
    "train_table = train_art.get(\"train_data\")\n",
    "\n",
    "# Get the images, ground truth label, and row index\n",
    "images = train_table.get_column(\"images\", convert_to=\"numpy\")\n",
    "labels = train_table.get_column(\"label_id\", convert_to=\"numpy\")\n",
    "ids = train_table.get_index()\n",
    "# Shuffle the ids and slice\n",
    "random.shuffle(ids)\n",
    "sample_ids = ids[0:viz_augment_samples]\n",
    "\n",
    "# Create augmentation table\n",
    "augment_table = wandb.Table(columns=['image', 'truth', 'label_id', 'aug1', 'aug2', 'aug3', 'aug4', 'aug5'])\n",
    "\n",
    "# Get augmented images and log it onto the table\n",
    "for sample_id in sample_ids:\n",
    "    img = images[sample_id]\n",
    "    label = labels[sample_id]\n",
    "    augmented_imgs = augment_5_times(tf.expand_dims(img, axis=0))\n",
    "    augment_table.add_data(wandb.Image(img),\n",
    "                           np.argmax(label),\n",
    "                           configs['class_names'][str(label)],\n",
    "                           augmented_imgs[0],\n",
    "                           augmented_imgs[1],\n",
    "                           augmented_imgs[2],\n",
    "                           augmented_imgs[3],\n",
    "                           augmented_imgs[4])\n",
    "\n",
    "# Log the table\n",
    "wandb.log({'augmented data': augment_table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def create_gradcam(image, model, last_conv_layer_name, pred_index=None):\n",
    "    # Preprocess the image array\n",
    "    image, _ = preprocess(tf.expand_dims(image, axis=0), 0)\n",
    "    # Get GradCAM\n",
    "    heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name, pred_index)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    jet_heatmap = tf.image.resize(jet_heatmap, size=(28,28))\n",
    "\n",
    "    # Overlay\n",
    "    superimposed_img = jet_heatmap * 0.4 + tf.squeeze(image, axis=0)\n",
    "    superimposed_img = tf.clip_by_value(superimposed_img, 0.0, 1.0)\n",
    "\n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'block4_conv3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_processor(ndx, row):\n",
    "    return {\n",
    "        \"input:image\": wandb.Image(row[\"input\"]),\n",
    "        \"target:class\": class_table.index_ref(row[\"target\"])\n",
    "    }\n",
    "\n",
    "def prediction_processor(ndx, row):\n",
    "    # Get the validation image\n",
    "    valid_image = np.array(row[\"val_row\"].get_row()[\"input:image\"].image)\n",
    "\n",
    "    return {\n",
    "        \"output:class\": class_table.index_ref(np.argmax(row[\"output\"])),\n",
    "        \"gradcam\": wandb.Image(create_gradcam(valid_image, model, last_conv_layer_name)),\n",
    "        \"output:logits\": {class_name: value for (class_name, value) in zip(list(config.class_names.values()), row[\"output\"].tolist())}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 6s 0us/step\n",
      "58900480/58889256 [==============================] - 6s 0us/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 3)]       0         \n",
      "                                                                 \n",
      " resizing (Resizing)         (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " img_augmentation (Sequentia  (None, None, None, 3)    0         \n",
      " l)                                                              \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 4, 4, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 2, 2, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 4104      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,718,792\n",
      "Trainable params: 14,718,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape: tuple=(28, 28, 3), \n",
    "              resize: tuple=(32, 32, 3),\n",
    "              dropout_rate: float=0.5,\n",
    "              num_classes: int=8,\n",
    "              output_activation: str='softmax'):\n",
    "  \n",
    "    inputs = layers.Input(input_shape)\n",
    "    resize_img = layers.Resizing(resize[0], resize[1], interpolation='bilinear')(inputs)\n",
    "    augment_img = img_augmentation(resize_img)\n",
    "  \n",
    "    base_model = tf.keras.applications.VGG16(include_top=False, \n",
    "                                             weights=configs['pretrain_weights'], \n",
    "                                             input_shape=resize,\n",
    "                                             input_tensor=augment_img)\n",
    "    base_model.trainabe = True\n",
    "\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=output_activation)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    # log the current learning rate onto W&B\n",
    "    if wandb.run is None:\n",
    "        raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n",
    "\n",
    "    wandb.log({'learning_rate': lr}, commit=False)\n",
    "    \n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-configs['lr_decay_rate'])\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, \n",
    "          callbacks: list,\n",
    "          verbose: int=0):\n",
    "    \"\"\"\n",
    "    Utility function to train the model.\n",
    "\n",
    "    Arguments:\n",
    "        config (dict): Dictionary of hyperparameters.\n",
    "        callbacks (list): List of callbacks passed to `model.fit`.\n",
    "        verbose (int): 0 for silent and 1 for progress bar.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initalize model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = get_model(resize=(config.image_width, config.image_height, config.image_channels))\n",
    "\n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate)\n",
    "    model.compile(opt,\n",
    "                  config.loss_fn,\n",
    "                  metrics=config.metrics)\n",
    "\n",
    "    # Train the model\n",
    "    _ = model.fit(trainloader,\n",
    "                  epochs=config.epochs,\n",
    "                  validation_data=validloader,\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hwx9xyz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858e511f92ff4c8d82cc19fb85d1e679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">leafy-violet-10</strong>: <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1hwx9xyz\" target=\"_blank\">https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1hwx9xyz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220614_205538-1hwx9xyz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hwx9xyz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ramg/Desktop/project3_CNN/CNN/wandb/run-20220614_205811-1ws05o9e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1ws05o9e\" target=\"_blank\">valiant-monkey-11</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 20:58:22.518071: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip: Could not satisfy explicit device specification '' because the node {{colocation_node model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nRngReadAndSkip: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_img_augmentation_random_rotation_stateful_uniform_rngreadandskip_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip (RngReadAndSkip) \n\n\t [[{{node model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip}}]] [Op:__inference_train_function_3766]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/6_rc724s369980pjcph8pltr0000gp/T/ipykernel_13282/3416449401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Evaluate the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/44/6_rc724s369980pjcph8pltr0000gp/T/ipykernel_13282/3159028371.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, callbacks, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     _ = model.fit(trainloader,\n\u001b[0m\u001b[1;32m     25\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensor-mac/lib/python3.9/site-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensor-mac/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensor-mac/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip: Could not satisfy explicit device specification '' because the node {{colocation_node model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \nColocation Debug Info:\nColocation group had the following types and supported devices: \nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\nRngReadAndSkip: CPU \n_Arg: GPU CPU \n\nColocation members, user-requested devices, and framework assigned devices, if any:\n  model_img_augmentation_random_rotation_stateful_uniform_rngreadandskip_resource (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\n  model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip (RngReadAndSkip) \n\n\t [[{{node model/img_augmentation/random_rotation/stateful_uniform/RngReadAndSkip}}]] [Op:__inference_train_function_3766]"
     ]
    }
   ],
   "source": [
    "# Initialize the W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', config=configs, job_type='train')\n",
    "config = wandb.config\n",
    "\n",
    "# Get validation table\n",
    "data_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "valid_table = data_art.get(\"valid_data\")\n",
    "\n",
    "# Create a class table\n",
    "class_table = wandb.Table(columns=[], data=[])\n",
    "class_table.add_column(\"class_name\", list(config.class_names.values()))\n",
    "\n",
    "# Define WandbCallback for experiment tracking\n",
    "wandb_callback = WandbCallback(\n",
    "                    log_evaluation=True,\n",
    "                    validation_row_processor=lambda ndx, row: validation_processor(ndx, row),\n",
    "                    prediction_row_processor=lambda ndx, row: prediction_processor(ndx, row),\n",
    "                    validation_steps=4,\n",
    "                    save_model=False\n",
    "                )\n",
    "\n",
    "# callbacks\n",
    "callbacks = [earlystopper, wandb_callback, lr_callback]\n",
    "\n",
    "# Train\n",
    "model = train(config, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluate the trained model\n",
    "loss, acc = model.evaluate(validloader)\n",
    "wandb.log({'evaluate/accuracy': acc})\n",
    "\n",
    "# Close the W&B run.\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('tensor2.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90364b22bc91425154ca545b5dd86b5ce89f674d5a5af9bdbf07597e1c5345b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
