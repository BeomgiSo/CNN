{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF:  2.6.0\n",
      "W&B:  0.12.18\n",
      "medMNIST:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TF: \", tf.__version__)\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "import wandb\n",
    "print(\"W&B: \", wandb.__version__)\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import medmnist\n",
    "print(\"medMNIST: \", medmnist.__version__)\n",
    "from medmnist import INFO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6735316972785188481,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 5736300544\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3036086298955826725\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbeomgiso\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict(\n",
    "    data_flag = 'bloodmnist',\n",
    "    image_width = 32,\n",
    "    image_height = 32,\n",
    "    batch_size = 128,\n",
    "    model_name = 'vgg16',\n",
    "    pretrain_weights = 'imagenet',\n",
    "    epochs = 100,\n",
    "    init_learning_rate = 0.001,\n",
    "    lr_decay_rate = 0.1,\n",
    "    optimizer = 'adam',\n",
    "    loss_fn = 'sparse_categorical_crossentropy',\n",
    "    metrics = ['acc'],\n",
    "    earlystopping_patience = 5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_class': 'BloodMNIST',\n",
       " 'description': 'The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set. The source images with resolution 3×360×363 pixels are center-cropped into 3×200×200, and then resized into 3×28×28.',\n",
       " 'url': 'https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1',\n",
       " 'MD5': '7053d0359d879ad8a5505303e11de1dc',\n",
       " 'task': 'multi-class',\n",
       " 'label': {'0': 'basophil',\n",
       "  '1': 'eosinophil',\n",
       "  '2': 'erythroblast',\n",
       "  '3': 'immature granulocytes(myelocytes, metamyelocytes and promyelocytes)',\n",
       "  '4': 'lymphocyte',\n",
       "  '5': 'monocyte',\n",
       "  '6': 'neutrophil',\n",
       "  '7': 'platelet'},\n",
       " 'n_channels': 3,\n",
       " 'n_samples': {'train': 11959, 'val': 1712, 'test': 3421},\n",
       " 'license': 'CC BY 4.0'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = INFO[configs['data_flag']]\n",
    "configs['class_names'] = info['label']\n",
    "configs['image_channels'] = info['n_channels']\n",
    "\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def download_and_prepare_dataset(data_info: dict):\n",
    "    \"\"\"\n",
    "    Utility function to download the dataset and return train/valid/test images/labels.\n",
    "\n",
    "    Arguments:\n",
    "        data_info (dict): Dataset metadata\n",
    "    \"\"\"\n",
    "    data_path = tf.keras.utils.get_file(origin=data_info['url'], md5_hash=data_info['MD5'])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get images\n",
    "        train_images = data['train_images']\n",
    "        valid_images = data['val_images']\n",
    "        test_images = data['test_images']\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data['train_labels'].flatten()\n",
    "        valid_labels = data['val_labels'].flatten()\n",
    "        test_labels = data['test_labels'].flatten()\n",
    "\n",
    "    return train_images, train_labels, valid_images, valid_labels, test_images, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, valid_images, valid_labels, test_images, test_labels = download_and_prepare_dataset(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train images : 1000 to be logged\n"
     ]
    }
   ],
   "source": [
    "# For demonstration purposes\n",
    "log_full = False #@param {type:\"boolean\"}\n",
    "\n",
    "if log_full:\n",
    "    log_train_samples = len(train_images)\n",
    "else:\n",
    "    log_train_samples = 1000 \n",
    "    \n",
    "\n",
    "print(f'Number of train images : {log_train_samples} to be logged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\beomgi_so\\Desktop\\CNN_project\\CNN\\wandb\\run-20220614_224102-1zdkgda2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1zdkgda2\" target=\"_blank\">cool-deluge-18</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cool-deluge-18</strong>: <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1zdkgda2\" target=\"_blank\">https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/1zdkgda2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220614_224102-1zdkgda2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3.52 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a new W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_data')\n",
    "\n",
    "# Intialize a W&B Artifacts\n",
    "ds = wandb.Artifact(\"medmnist_bloodmnist_dataset\", \"dataset\")\n",
    "\n",
    "# Initialize an empty table\n",
    "train_table = wandb.Table(columns=[], data=[])\n",
    "# Add training data\n",
    "train_table.add_column('image', train_images[:log_train_samples])\n",
    "# Add training label_id\n",
    "train_table.add_column('label_id', train_labels[:log_train_samples])\n",
    "# Add training class names\n",
    "train_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_names\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "\n",
    "# Add the table to the Artifact\n",
    "ds['train_data'] = train_table\n",
    "\n",
    "# Let's do the same for the validation data\n",
    "valid_table = wandb.Table(columns=[], data=[])\n",
    "valid_table.add_column('image', valid_images)\n",
    "valid_table.add_column('label_id', valid_labels)\n",
    "valid_table.add_computed_columns(lambda ndx, row:{\n",
    "    \"images\": wandb.Image(row[\"image\"]),\n",
    "    \"class_name\": configs['class_names'][str(row[\"label_id\"])]\n",
    "    })\n",
    "ds['valid_data'] = valid_table\n",
    "\n",
    "# Save the dataset as an Artifact\n",
    "ds.save()\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "@tf.function\n",
    "def preprocess(image: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Preprocess the image tensors and parse the labels\n",
    "    \"\"\"\n",
    "    # Preprocess images\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    \n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(images: np.ndarray,\n",
    "                       labels: np.ndarray,\n",
    "                       loader_type: str='train',\n",
    "                       batch_size: int=128):\n",
    "    \"\"\"\n",
    "    Utility function to prepare dataloader.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    if loader_type=='train':\n",
    "        dataset = dataset.shuffle(1024)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = prepare_dataloader(train_images, train_labels, 'train', configs.get('batch_size', 64))\n",
    "validloader = prepare_dataloader(valid_images, valid_labels, 'valid', configs.get('batch_size', 64))\n",
    "testloader = prepare_dataloader(test_images, test_labels, 'test', configs.get('batch_size', 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_augmentation = models.Sequential(\n",
    "    [\n",
    "        layers.RandomRotation(factor=0.15),\n",
    "        layers.RandomFlip()],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def augment_5_times(img):\n",
    "    augmented_imgs = []\n",
    "    for _ in range(5):\n",
    "        aug_img = tf.squeeze(img_augmentation(img), axis=0)\n",
    "        # Notice the use of wrapping the images with wandb.Image\n",
    "        wandb_image = wandb.Image(aug_img.numpy())\n",
    "        augmented_imgs.append(wandb_image)\n",
    "\n",
    "    return augmented_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\beomgi_so\\Desktop\\CNN_project\\CNN\\wandb\\run-20220614_224125-3lbukufr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3lbukufr\" target=\"_blank\">celestial-sunset-19</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.112 MB of 1.119 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.992962…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">celestial-sunset-19</strong>: <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3lbukufr\" target=\"_blank\">https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/3lbukufr</a><br/>Synced 5 W&B file(s), 1 media file(s), 601 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220614_224125-3lbukufr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.86 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "viz_augment_samples = 100\n",
    "\n",
    "# Initialize a W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', group='viz_augmentation')\n",
    "\n",
    "# Use the already logged dataset\n",
    "train_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "\n",
    "# Get the train_table to access the data\n",
    "train_table = train_art.get(\"train_data\")\n",
    "\n",
    "# Get the images, ground truth label, and row index\n",
    "images = train_table.get_column(\"images\", convert_to=\"numpy\")\n",
    "labels = train_table.get_column(\"label_id\", convert_to=\"numpy\")\n",
    "ids = train_table.get_index()\n",
    "# Shuffle the ids and slice\n",
    "random.shuffle(ids)\n",
    "sample_ids = ids[0:viz_augment_samples]\n",
    "\n",
    "# Create augmentation table\n",
    "augment_table = wandb.Table(columns=['image', 'truth', 'label_id', 'aug1', 'aug2', 'aug3', 'aug4', 'aug5'])\n",
    "\n",
    "# Get augmented images and log it onto the table\n",
    "for sample_id in sample_ids:\n",
    "    img = images[sample_id]\n",
    "    label = labels[sample_id]\n",
    "    augmented_imgs = augment_5_times(tf.expand_dims(img, axis=0))\n",
    "    augment_table.add_data(wandb.Image(img),\n",
    "                           np.argmax(label),\n",
    "                           configs['class_names'][str(label)],\n",
    "                           augmented_imgs[0],\n",
    "                           augmented_imgs[1],\n",
    "                           augmented_imgs[2],\n",
    "                           augmented_imgs[3],\n",
    "                           augmented_imgs[4])\n",
    "\n",
    "# Log the table\n",
    "wandb.log({'augmented data': augment_table})\n",
    "\n",
    "# Finish the run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def create_gradcam(image, model, last_conv_layer_name, pred_index=None):\n",
    "    # Preprocess the image array\n",
    "    image, _ = preprocess(tf.expand_dims(image, axis=0), 0)\n",
    "    # Get GradCAM\n",
    "    heatmap = make_gradcam_heatmap(image, model, last_conv_layer_name, pred_index)\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    jet_heatmap = tf.image.resize(jet_heatmap, size=(28,28))\n",
    "\n",
    "    # Overlay\n",
    "    superimposed_img = jet_heatmap * 0.4 + tf.squeeze(image, axis=0)\n",
    "    superimposed_img = tf.clip_by_value(superimposed_img, 0.0, 1.0)\n",
    "\n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'block4_conv3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_processor(ndx, row):\n",
    "    return {\n",
    "        \"input:image\": wandb.Image(row[\"input\"]),\n",
    "        \"target:class\": class_table.index_ref(row[\"target\"])\n",
    "    }\n",
    "\n",
    "def prediction_processor(ndx, row):\n",
    "    # Get the validation image\n",
    "    valid_image = np.array(row[\"val_row\"].get_row()[\"input:image\"].image)\n",
    "\n",
    "    return {\n",
    "        \"output:class\": class_table.index_ref(np.argmax(row[\"output\"])),\n",
    "        \"gradcam\": wandb.Image(create_gradcam(valid_image, model, last_conv_layer_name)),\n",
    "        \"output:logits\": {class_name: value for (class_name, value) in zip(list(config.class_names.values()), row[\"output\"].tolist())}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 3)]       0         \n",
      "_________________________________________________________________\n",
      "resizing (Resizing)          (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "img_augmentation (Sequential (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 14,718,792\n",
      "Trainable params: 14,718,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model(input_shape: tuple=(28, 28, 3), \n",
    "              resize: tuple=(32, 32, 3),\n",
    "              dropout_rate: float=0.5,\n",
    "              num_classes: int=8,\n",
    "              output_activation: str='softmax'):\n",
    "  \n",
    "    inputs = layers.Input(input_shape)\n",
    "    resize_img = layers.Resizing(resize[0], resize[1], interpolation='bilinear')(inputs)\n",
    "    augment_img = img_augmentation(resize_img)\n",
    "  \n",
    "    base_model = tf.keras.applications.VGG16(include_top=False, \n",
    "                                             weights=configs['pretrain_weights'], \n",
    "                                             input_shape=resize,\n",
    "                                             input_tensor=augment_img)\n",
    "    base_model.trainabe = True\n",
    "\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=output_activation)(x)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=configs['earlystopping_patience'], verbose=0, mode='auto',\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr):\n",
    "    # log the current learning rate onto W&B\n",
    "    if wandb.run is None:\n",
    "        raise wandb.Error(\"You must call wandb.init() before WandbCallback()\")\n",
    "\n",
    "    wandb.log({'learning_rate': lr}, commit=False)\n",
    "    \n",
    "    if epoch < 7:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-configs['lr_decay_rate'])\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, \n",
    "          callbacks: list,\n",
    "          verbose: int=0):\n",
    "    \"\"\"\n",
    "    Utility function to train the model.\n",
    "\n",
    "    Arguments:\n",
    "        config (dict): Dictionary of hyperparameters.\n",
    "        callbacks (list): List of callbacks passed to `model.fit`.\n",
    "        verbose (int): 0 for silent and 1 for progress bar.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initalize model\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = get_model(resize=(config.image_width, config.image_height, config.image_channels))\n",
    "\n",
    "    # Compile the model\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config.init_learning_rate)\n",
    "    model.compile(opt,\n",
    "                  config.loss_fn,\n",
    "                  metrics=config.metrics)\n",
    "\n",
    "    # Train the model\n",
    "    _ = model.fit(trainloader,\n",
    "                  epochs=config.epochs,\n",
    "                  validation_data=validloader,\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.18"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\beomgi_so\\Desktop\\CNN_project\\CNN\\wandb\\run-20220614_224158-2z0bivca</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist/runs/2z0bivca\" target=\"_blank\">easy-grass-20</a></strong> to <a href=\"https://wandb.ai/beomgiso/medmnist-bloodmnist\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "# Initialize the W&B run\n",
    "run = wandb.init(project='medmnist-bloodmnist', config=configs, job_type='train')\n",
    "config = wandb.config\n",
    "\n",
    "# Get validation table\n",
    "data_art = run.use_artifact('ayush-thakur/medmnist-bloodmnist/medmnist_bloodmnist_dataset:latest', type='dataset')\n",
    "valid_table = data_art.get(\"valid_data\")\n",
    "\n",
    "# Create a class table\n",
    "class_table = wandb.Table(columns=[], data=[])\n",
    "class_table.add_column(\"class_name\", list(config.class_names.values()))\n",
    "\n",
    "# Define WandbCallback for experiment tracking\n",
    "wandb_callback = WandbCallback(\n",
    "                    log_evaluation=True,\n",
    "                    validation_row_processor=lambda ndx, row: validation_processor(ndx, row),\n",
    "                    prediction_row_processor=lambda ndx, row: prediction_processor(ndx, row),\n",
    "                    validation_steps=4,\n",
    "                    save_model=False\n",
    "                )\n",
    "\n",
    "# callbacks\n",
    "callbacks = [earlystopper, wandb_callback, lr_callback]\n",
    "\n",
    "# Train\n",
    "model = train(config, callbacks=callbacks, verbose=1)\n",
    "\n",
    "# Evaluate the trained model\n",
    "loss, acc = model.evaluate(validloader)\n",
    "wandb.log({'evaluate/accuracy': acc})\n",
    "\n",
    "# Close the W&B run.\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5ebc6cf6f7fb120347a232b371902f436be28b211613027002426b5b4c265db"
  },
  "kernelspec": {
   "display_name": "project3",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
