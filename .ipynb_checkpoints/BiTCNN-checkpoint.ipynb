{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import tensorflow as tf\n",
    "#일반화된 문제들에 대해서 모델의 재사용을 극대화 하기위해 구글에서 제공한 API\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 16:26:07.736240: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-14 16:26:07.736385: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Load model into KerasLayer\n",
    "model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
    "module = hub.KerasLayer(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#@title tf_flowers label names (hidden)\n",
    "tf_flowers_labels = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
    "NUM_CLASSES=len(tf_flowers_labels)\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new head to the BiT model\n",
    "\n",
    "class MyBiTModel(tf.keras.Model):\n",
    "  \"\"\"BiT with a new head.\"\"\"\n",
    "\n",
    "  def __init__(self, num_classes, module):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_classes = num_classes\n",
    "    self.head = tf.keras.layers.Dense(num_classes, kernel_initializer='zeros')\n",
    "    self.bit_model = module\n",
    "  \n",
    "  def call(self, images):\n",
    "    # No need to cut head off since we are using feature extractor model\n",
    "    bit_embedding = self.bit_model(images)\n",
    "    return self.head(bit_embedding)\n",
    "\n",
    "model = MyBiTModel(num_classes=NUM_CLASSES, module=module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Set dataset-dependent hyperparameters\n",
    "\n",
    "#@markdown Here we set dataset-dependent hyperparameters. For example, our dataset of flowers has 3670 images of varying size (a few hundred x a few hundred pixels), so the image size is larger than 96x96 and the dataset size is <20k examples. However, for speed reasons (since this is a tutorial and we are training on a single GPU), we will select the `<96x96 px` option and train on lower resolution images. As we will see, we can still attain strong results.\n",
    "\n",
    "#@markdown **Algorithm details: how are the hyperparameters dataset-dependent?** \n",
    "\n",
    "#@markdown It's quite intuitive - we resize images to a smaller fixed size if they are smaller than 96 x 96px and to a larger fixed size otherwise. The number of steps we fine-tune for is larger for larger datasets. \n",
    "\n",
    "IMAGE_SIZE = \"=\\u003C96x96 px\" #@param [\"=<96x96 px\",\"> 96 x 96 px\"]\n",
    "DATASET_SIZE = \"\\u003C20k examples\" #@param [\"<20k examples\", \"20k-500k examples\", \">500k examples\"]\n",
    "\n",
    "if IMAGE_SIZE == \"=<96x96 px\":\n",
    "  RESIZE_TO = 160\n",
    "  CROP_TO = 128\n",
    "else:\n",
    "  RESIZE_TO = 512\n",
    "  CROP_TO = 480\n",
    "\n",
    "if DATASET_SIZE == \"<20k examples\":\n",
    "  SCHEDULE_LENGTH = 500\n",
    "  SCHEDULE_BOUNDARIES = [200, 300, 400]\n",
    "elif DATASET_SIZE == \"20k-500k examples\":\n",
    "  SCHEDULE_LENGTH = 10000\n",
    "  SCHEDULE_BOUNDARIES = [3000, 6000, 9000]\n",
    "else:\n",
    "  SCHEDULE_LENGTH = 20000\n",
    "  SCHEDULE_BOUNDARIES = [6000, 12000, 18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimiser and loss\n",
    "\n",
    "# BATCH_SIZE = 32\n",
    "# lr = 0.003 * BATCH_SIZE / 512 \n",
    "\n",
    "SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n",
    "lr = 0.003 * BATCH_SIZE / 512\n",
    "\n",
    "# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES, \n",
    "                                                                   values=[lr, lr*0.1, lr*0.001, lr*0.0001])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/44/6_rc724s369980pjcph8pltr0000gp/T/ipykernel_1420/532448448.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m pipeline_train = (ds_train\n\u001b[0m\u001b[1;32m     28\u001b[0m                   \u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                   \u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCHEDULE_LENGTH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mDATASET_NUM_TRAIN_EXAMPLES\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# repeat dataset_size / num_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocessing helper functions\n",
    "\n",
    "# Create data pipelines for training and testing:\n",
    "BATCH_SIZE = 512\n",
    "SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\n",
    "\n",
    "STEPS_PER_EPOCH = 10\n",
    "\n",
    "def cast_to_tuple(features):\n",
    "  return (features['image'], features['label'])\n",
    "  \n",
    "def preprocess_train(features):\n",
    "  # Apply random crops and horizontal flips for all tasks \n",
    "  # except those for which cropping or flipping destroys the label semantics\n",
    "  # (e.g. predict orientation of an object)\n",
    "  features['image'] = tf.image.random_flip_left_right(features['image'])\n",
    "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\n",
    "  features['image'] = tf.image.random_crop(features['image'], [CROP_TO, CROP_TO, 3])\n",
    "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
    "  return features\n",
    "\n",
    "def preprocess_test(features):\n",
    "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\n",
    "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
    "  return features\n",
    "\n",
    "pipeline_train = (ds_train\n",
    "                  .shuffle(10000)\n",
    "                  .repeat(int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH) + 1 + 50)  # repeat dataset_size / num_steps\n",
    "                  .map(preprocess_train, num_parallel_calls=8)\n",
    "                  .batch(BATCH_SIZE)\n",
    "                  .map(cast_to_tuple)  # for keras model.fit\n",
    "                  .prefetch(2))\n",
    "\n",
    "pipeline_test = (ds_test.map(preprocess_test, num_parallel_calls=1)\n",
    "                  .map(cast_to_tuple)  # for keras model.fit\n",
    "                  .batch(BATCH_SIZE)\n",
    "                  .prefetch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune model\n",
    "history = model.fit(\n",
    "    pipeline_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs= int(SCHEDULE_LENGTH / STEPS_PER_EPOCH),  # TODO: replace with `epochs=10` here to shorten fine-tuning for tutorial if you wish\n",
    "    validation_data=pipeline_test  # here we are only using \n",
    "                                   # this data to evaluate our performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model as SavedModel\n",
    "export_module_dir = '/tmp/my_saved_bit_model/'\n",
    "tf.saved_model.save(model, export_module_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "saved_module = hub.KerasLayer(export_module_dir, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions from new model\n",
    "for features in ds_train.take(1):\n",
    "  image = features['image']\n",
    "  image = preprocess_image(image)\n",
    "  image = tf.image.resize(image, [CROP_TO, CROP_TO])\n",
    "\n",
    "  # Run model on image\n",
    "  logits = saved_module(image)\n",
    "  \n",
    "  # Show image and predictions\n",
    "  show_preds(logits, image[0], correct_flowers_label=features['label'].numpy(), tf_flowers_logits=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5ebc6cf6f7fb120347a232b371902f436be28b211613027002426b5b4c265db"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
